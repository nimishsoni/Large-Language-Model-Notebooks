# Large-Language-Model-Notebooks
Repo hosting Large Language Model experimentation notebooks
#### https://github.com/nimishsoni/Large-Language-Model-Notebooks/blob/main/Chatbot_Prompt_Engineering_SQLTraslator_OpenAI_API.ipynb
  Key Libraries used: Transformers, Langchain
  The notebook has code to create a Chatbot: that interacts with user to get order details for a restaurant, a SQL Translator that converts natural language queries to SQL, and Prompt Engineering using OpenAI API.

#### https://github.com/nimishsoni/Large-Language-Model-Notebooks/blob/main/LangChain_Agent_create_Data_Scientist_Assistant.ipynb
Key Libraries used: Transformers, LangChain
  Notebook to create a data scientist assistant using LangChain. Given a complex data science task the assistant solves it in logical steps through exploration, pre-processing, and modeling. 

#### https://github.com/nimishsoni/Large-Language-Model-Notebooks/blob/main/Moderation_system_with_LLAMA_2_LangChain_and_HF.ipynb
Key Libraries used: Transformers, LangChain
  In this notebook, we create an automated customer service assistant that replies to customer complaints in an automated way. The following are the implementation steps:
  Use TinyLama model and tokenizer pretrained and available on HF
- Use HuggingFace Pipeline to create an assistant model
- Create and provide the prompt template for the assistant model which gives a template for the expected response and create a chain using Langchain
- Create a moderator (using template and same model) that moderates the output generated by the assistant model and removes any negative comments generated.
- Chain the assistant and moderator output together.

#### https://github.com/nimishsoni/Large-Language-Model-Notebooks/blob/main/PEFT_LoRA.ipynb
Libraries Used: datasets, transformers, peft. Model Used: pretrained bloom 560M model from HF. The dataset used for fine-tuning the model: awesome chatgpt prompts
This notebook implements Low-rank adaptation of a large language model to speed-up fine-tuning of the model on prompt dataset
- Load pretrained tokenizer and causal language model Bloom - 560m from HF
- Load and preprocess chatgpt prompt dataset
- Use PEFT library to define peft model using (bloom) and LoRA configuration parameters
- Use transformer trainer function to fine-tune the peft-model using LoRA configuration.
- Use the trained model for generating prompts.

#### https://github.com/nimishsoni/Large-Language-Model-Notebooks/blob/main/PEFT_Prompt_Tuning.ipynb
Libraries Used: datasets, transformers, peft. Model Used: pretrained bloom 560M model from HF. The dataset used for fine-tuning the model: awesome chatgpt prompts, Abirate/english_quotes
This notebook implements prompt tuning for a large language model by modifying the weights of the prompt. To achieve that, we must add some new values to the prompt, and these values are trained. We only modify the weights of the new values in the layers containing the prompt.

We can modify the behavior of a model by just updating 0.0005% of their weights. Achieving a similar result to other techniques where we update the weights of the model.
- Load pretrained tokenizer and causal language model Bloom - 560m from HF
- Load and preprocess chatgpt prompt and English quotes dataset
- Use PEFT library to define peft model using (bloom) and Prompt Tuning configuration parameters
- Use transformer trainer function to fine-tune the peft-model using Prompt Tuning configuration
- Use the fine-tuned model for generating prompts and quotes

#### https://github.com/nimishsoni/Large-Language-Model-Notebooks/blob/main/PEFT_QLoRA.ipynb
Libraries Used: datasets, transformers, peft. Model used: pretrained bloom 560M model from HF. The dataset used for fine-tuning the QLoRA Tuning using PEFT from Hugging Face.





